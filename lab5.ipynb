{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561fb79b-df2b-4f57-aca7-acf4f098905a",
   "metadata": {},
   "source": [
    "一.数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b95ef4-2314-49c9-87e6-8799530f277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载图片数据\n",
    "import os\n",
    "from PIL import Image\n",
    "imgs=[]\n",
    "for i in range(1,5130):\n",
    "    path='./data/'+str(i)+'.jpg'\n",
    "    if os.path.exists(path):\n",
    "        img=Image.open(path)   \n",
    "        img=img.resize((227,227))\n",
    "        imgs.append((i,img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcc92c1-a1a8-41ca-8d1d-50e5f03e1efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-4f612adcdbe8>:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  b=torch.tensor(a)\n"
     ]
    }
   ],
   "source": [
    "#将图片数据转化为tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "a=[]\n",
    "for i,j in imgs:\n",
    "    a.append(np.array(j))\n",
    "b=torch.tensor(a)\n",
    "imgs_vector=[]\n",
    "for i in range(4869):\n",
    "    imgs_vector.append((imgs[i][0],b[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecff5923-078a-4b68-83f7-5ec5fa09f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载文本数据\n",
    "sentences=[]\n",
    "for i in range(1,5130):\n",
    "    path='./data/'+str(i)+'.txt'\n",
    "    if os.path.exists(path):\n",
    "        with open(path,'r',errors='ignore') as f:\n",
    "            sentence=f.read()\n",
    "            sentences.append((i,sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2a7cee-b477-4798-a33f-0ed9968cc238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#组合图片数据和文本数据\n",
    "imgs_vector_=[]\n",
    "for i in range(4869):\n",
    "    imgs_vector_.append((imgs_vector[i][0],imgs_vector[i][1].transpose(0,2)*1.0))\n",
    "data=[]\n",
    "for i in range(4869):\n",
    "    data.append((imgs_vector[i][0],imgs_vector_[i][1],sentences[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b70b2fba-cb45-41b6-9573-598e24caa74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集和测试集\n",
    "train=[]\n",
    "with open('train.txt','r') as f:\n",
    "    for line in f.readlines():\n",
    "        line=line.strip('\\n')  \n",
    "        train.append(line)\n",
    "train_=[]\n",
    "for i in train:\n",
    "    if i!='guid,tag':\n",
    "        a=i.index(',')\n",
    "        train_.append((int(i[:a]),i[a+1:]))\n",
    "train__=[]\n",
    "for i in train_:\n",
    "    if i[1]=='negative':\n",
    "        train__.append((i[0],0))\n",
    "    if i[1]=='neutral':\n",
    "        train__.append((i[0],1))\n",
    "    if i[1]=='positive':\n",
    "        train__.append((i[0],2))\n",
    "test=[]\n",
    "with open('test_without_label.txt','r') as f:\n",
    "    for line in f.readlines():\n",
    "        line=line.strip('\\n')  \n",
    "        test.append(line)\n",
    "test_=[]\n",
    "for i in test:\n",
    "    if i!='guid,tag':\n",
    "        a=i.index(',')\n",
    "        test_.append((int(i[:a]),-1))\n",
    "dict_train={}\n",
    "for i in train__:\n",
    "    dict_train[i[0]]=i[1]\n",
    "dict_test={}\n",
    "for i in test_:\n",
    "    dict_test[i[0]]=-1\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "for i in data:\n",
    "    if i[0] in dict_train:\n",
    "        train_data.append((i[0],i[1],i[2],dict_train[i[0]]))\n",
    "    if i[0] in dict_test:\n",
    "        test_data.append((i[0],i[1],i[2],dict_test[i[0]]))\n",
    "    else: \n",
    "        continue      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a94b7b42-382a-4f8c-9cf1-f8f535a2afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将文本数据转化为tensor\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "train_sentences=[]\n",
    "for i in train_data:\n",
    "    train_sentences.append(i[2])\n",
    "train_sentences_fenci=[]\n",
    "for train_sentence in train_sentences:\n",
    "    train_sentences_fenci.append(nltk.word_tokenize(train_sentence))\n",
    "train_sentences_quci=[]\n",
    "stop_words=stopwords.words('english')\n",
    "for i in [',','.',';','?','!','#','&','(',')','@',':','-','...','``','....','[',']','http','“','--','/…',\"''\",\"'\",'..','$','%','…','–','‐‐‐','=','<','>','[]','http…','’','~','RT']:\n",
    "    stop_words.append(i)\n",
    "for train_sentence in train_sentences_fenci:\n",
    "    quci=[i for i in train_sentence if i not in stop_words and i[:2]!='//']\n",
    "    train_sentences_quci.append(quci)  \n",
    "train_sentences_small=[]\n",
    "for train_sentence in train_sentences_quci:\n",
    "    train_sentences_small_=[]\n",
    "    for i in train_sentence:\n",
    "         train_sentences_small_.append(i.lower())\n",
    "    train_sentences_small.append(train_sentences_small_)\n",
    "model1=Word2Vec(train_sentences_small,sg=1,window=5,min_count=5,negative=3,sample=0.001,hs=1,workers=4)\n",
    "words=list(model1.wv.index_to_key)\n",
    "train_set=[]\n",
    "for i in range(4000):\n",
    "    if len(train_sentences_small[i])==0:\n",
    "        train_set.append((train_data[i][0],train_data[i][1],np.zeros(100),train_data[i][3]))\n",
    "    else:\n",
    "        a=np.zeros(100)\n",
    "        for j in train_sentences_small[i]:\n",
    "            if j in words:\n",
    "                a+=model1.wv[j]\n",
    "        a=a/len(train_sentences_small[i])\n",
    "        train_set.append((train_data[i][0],train_data[i][1],a,train_data[i][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d689b254-f4eb-43c3-b735-f5525bed67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#从训练集中划分训练集和验证集\n",
    "train_sets=train_set[:3000]\n",
    "dev_sets=train_set[3000:]\n",
    "train_sets=torch.utils.data.DataLoader(train_sets,batch_size=64,shuffle=True)      \n",
    "dev_sets=torch.utils.data.DataLoader(dev_sets,batch_size=64,shuffle=True)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb276c-ac5d-4bf7-82e4-377746c510a8",
   "metadata": {},
   "source": [
    "二.搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2575c441-0244-4413-889f-437649ea42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.conv1=nn.Sequential(                                                                       \n",
    "        nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4,padding=0,bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "        nn.LocalResponseNorm(size=5,alpha=0.0001,beta=0.75)\n",
    "                                 )\n",
    "        self.conv2=nn.Sequential(                                                                       \n",
    "        nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5,stride=1,padding=2,bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "        nn.LocalResponseNorm(size=5,alpha=0.0001,beta=0.75) \n",
    "                                 )\n",
    "        self.conv3=nn.Sequential(                                                                        \n",
    "        nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "        nn.ReLU()       \n",
    "                                 )        \n",
    "        self.conv4=nn.Sequential(                                                                                 \n",
    "        nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "        nn.ReLU()                  \n",
    "                                 )              \n",
    "        self.conv5=nn.Sequential(                                                                        \n",
    "        nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3,stride=2)      \n",
    "                                 )                                                                                                                 #网络第六层                           \n",
    "        self.flatten=nn.Flatten()\n",
    "        self.f1=nn.Sequential(                                                                             \n",
    "        nn.Linear(9216,4096),                                                            \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout()\n",
    "                              )\n",
    "        self.f2=nn.Sequential(                                                                           \n",
    "        nn.Linear(4096,4096),                                                            \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout()      \n",
    "                              )\n",
    "        self.f3=nn.Linear(4096,64)      \n",
    "        self.f4=nn.Sequential(                                                                           \n",
    "        nn.Linear(100,100),                                                            \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),  \n",
    "        nn.Linear(100,64)\n",
    "                              )    \n",
    "        self.f5=nn.Sequential(                                                                           \n",
    "        nn.Linear(128,32),                                                            \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),  \n",
    "        nn.Linear(32,3)\n",
    "                              )        \n",
    "    def forward(self,img,sentence):                                                                                                       \n",
    "        img=self.conv1(img)\n",
    "        img=self.conv2(img)\n",
    "        img=self.conv3(img)\n",
    "        img=self.conv4(img)\n",
    "        img=self.conv5(img)\n",
    "        img=self.flatten(img)\n",
    "        img=self.f1(img)\n",
    "        img=self.f2(img)\n",
    "        img=self.f3(img)\n",
    "        sentence=sentence.to(torch.float32)\n",
    "        sentence=self.f4(sentence)\n",
    "        feature=torch.cat([img,sentence],1)\n",
    "        pred=self.f5(feature)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5dc584-1f82-472c-8590-00d322b2aedf",
   "metadata": {},
   "source": [
    "三.训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c28189a1-f162-4161-bc1d-e11a97979a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 dev_loss:0.937666 dev_acc:0.5500\n"
     ]
    }
   ],
   "source": [
    "epoch=5                                                                                     \n",
    "model2=Model()                                                                                      \n",
    "optimizer=torch.optim.SGD(model2.parameters(),lr=0.01)                                                \n",
    "loss_f=nn.CrossEntropyLoss()                                                                       \n",
    "for epoch in range(1,epoch+1):                                   \n",
    "    for train_index,train_img,train_sentence,train_label in train_sets:\n",
    "        y_hat=model2(train_img,train_sentence) \n",
    "        loss=loss_f(y_hat,train_label)\n",
    "        optimizer.zero_grad()                                                                        \n",
    "        loss.backward()                                                                              \n",
    "        optimizer.step()                                                                            \n",
    "    dev_loss=0                                                                                      \n",
    "    dev_acc=0                                                                                    \n",
    "    for dev_index,dev_img,dev_sentence,dev_label in dev_sets  : \n",
    "        y_hat=model2(dev_img,dev_sentence)                                                                         \n",
    "        dev_loss+=loss_f(y_hat,dev_label).item()*dev_index.size(0)                                                   \n",
    "        dev_acc+=(y_hat.argmax(dim=1)==dev_label).float().sum().item()                 \n",
    "    print('epoch:%d dev_loss:%.6f dev_acc:%.4f'%(epoch,dev_loss/1000,dev_acc/1000))                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03682f-212e-433d-b083-97dba5eafa6d",
   "metadata": {},
   "source": [
    "四.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81155ad0-b868-41c4-9956-7b0f3b8f7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences=[]\n",
    "for i in test_data:\n",
    "    test_sentences.append(i[2])\n",
    "test_sentences_fenci=[]\n",
    "for test_sentence in test_sentences:\n",
    "    test_sentences_fenci.append(nltk.word_tokenize(test_sentence))\n",
    "test_sentences_quci=[]\n",
    "for test_sentence in test_sentences_fenci:\n",
    "    quci=[i for i in test_sentence if i not in stop_words and i[:2]!='//']\n",
    "    test_sentences_quci.append(quci)  \n",
    "test_sentences_small=[]\n",
    "for test_sentence in test_sentences_quci:\n",
    "    test_sentences_small_=[]\n",
    "    for i in test_sentence:\n",
    "         test_sentences_small_.append(i.lower())\n",
    "    test_sentences_small.append(test_sentences_small_)\n",
    "test_set=[]\n",
    "for i in range(511):\n",
    "    if len(test_sentences_small[i])==0:\n",
    "        test_set.append((test_data[i][0],test_data[i][1],np.zeros(100),test_data[i][3]))\n",
    "    else:\n",
    "        a=np.zeros(100)\n",
    "        for j in test_sentences_small[i]:\n",
    "            if j in words:\n",
    "                a+=model1.wv[j]\n",
    "        a=a/len(test_sentences_small[i])\n",
    "        test_set.append((test_data[i][0],test_data[i][1],a,test_data[i][3]))\n",
    "test_set=torch.utils.data.DataLoader(test_set,batch_size=64,shuffle=True)\n",
    "pred=[]\n",
    "for test_index,test_img,test_sentence,test_label in test_set: \n",
    "        y_hat=model2(test_img,test_sentence)\n",
    "        for i in range(len(test_index)):\n",
    "            pred.append((test_index[i],y_hat.argmax(dim=1)[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7ece4-2c90-4f83-9789-35eef0504171",
   "metadata": {},
   "source": [
    "五.写入预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d93da08b-68b9-459f-9b8e-252b7da67e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in test_:\n",
    "    a.append(i[0])\n",
    "b={}\n",
    "for i in pred:\n",
    "    b[int(i[0])]=int(i[1])\n",
    "c=['guid,tag']\n",
    "for i in a:\n",
    "    if b[i]==0:\n",
    "        c.append(str(i)+','+'negative')\n",
    "    if b[i]==1:\n",
    "        c.append(str(i)+','+'neutral')\n",
    "    if b[i]==2:\n",
    "        c.append(str(i)+','+'positive')\n",
    "with open('pred.txt','w') as f:\n",
    "    for i in c:\n",
    "        f.write(i+'\\n')\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
